{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT8 Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FT8 digital mode uses a variety of codes to enable reliable transmission of information in the presence of noise.\n",
    "These include:\n",
    "\n",
    "1. A LDPC (low-density parity-check) code which is used for error detection and correction.\n",
    "1. A CRC (cyclic redundancy check) code which is used for error detection.\n",
    "1. A Gray code which is used to minimise the introduction of errors when converting from the analog to the digital domain.\n",
    "\n",
    "This notebook captures some background on each of these codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDPC Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand LDPC codes we first need a little bit of background on block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Block Coding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block coding is a technique used to detect and sometimes correct errors that may occur when sending information \n",
    "in the form of digital bits across a communication channel.\n",
    "Practical block codes that could both detect and correct errors were first developed in the late 1940s\n",
    "by Golay [[1](#References)] and Hamming [[2](#References)].\n",
    "\n",
    "Block coding operates using the following process:\n",
    "\n",
    "1. The information to be transmitted is broken into fixed size blocks each containing $k$ bits of information.\n",
    "1. An encoder takes the $k$ bits of information in each block and generates $r$ redundant bits which are then combined with the original $k$ bits of information to create an $n$ bit codeword where $n = k+r$.\n",
    "1. The $n$ bit codeword is transmitted across the noisy communication channel using an appropriate modulation and demodulation method.\n",
    "1. A decoder takes the output of the demodulator and attempts to reconstruct the original $k$ bits of information using the the received $n$ bit codeword which may have been corrupted by the noisy communication channel.\n",
    "\n",
    "The performance of a particular block code can be described using several different measures:\n",
    "\n",
    "1. The bandwidth efficiency or rate of the code, which a measure of how much the communication channel bandwidth needs to increase in order to accomodate the $r$ bits of redundant information added by the encoder - this is expressed by the the ratio $k/n$ where $k$ is the number of information bits in each block and $n$ is the number\n",
    "of bits in the coresponding codeword for each block\n",
    "1. The complexity of the encoding and decoding processes, which is a measure of how much computation is required to perform encoding and decoding operations\n",
    "1. The error rate at the output of the decoder as a function of the communication channel signal to noise ratio - this is usually expressed by a graph of decoder output bit error rate vs communication channel signal to noise ratio for a specific type of communication channel\n",
    "\n",
    "The goal of block code design is to optimise the code rate (make it high as possible), complexity (make it low as possible) and error rate (make it low as possible) within the constraints of a given system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a LDPC Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDPC (low-density parity-check) codes are a family of parity-checking linear block codes first described\n",
    "in the early 1960s by Gallager [[3](#References)].\n",
    "Gallager original thesis included details on how both LDPC encoders and LDPC decoders could be constructed.\n",
    "However, in the early 1960s, cost effective construction of systems based on LDPC codes (and the simulations needed to design them) was beyond the capability of the available technology.\n",
    "As a consequence, LDPC codes were mostly forgotten for many years before being rediscovered in the 1990s\n",
    "[[4](#References)].\n",
    "LDPC are now widely used, having been incorporated into several communications standards.\n",
    "\n",
    "Parity-checking linear block codes are defined by a series of linear parity-check equations that relate the $r$ redundant parity bits to the $k$ information bits in each $n$ bit codeword.\n",
    "Each linear equation involves the addition (modulo 2) of a subset of the $k$ information bits and a subset of\n",
    "the $r$ redundant parity bits such that the addition (modulo 2) equals zero.\n",
    "\n",
    "These linear parity-check equations can by expressed in matrix form using a parity-check matrix that is designated in the literature by the symbol $H$.\n",
    "The parity-check matrix has $m$ rows and $n$ columns, where $m$ is the number of parity-check equations and\n",
    "$n$ is the number of bits in each codeword.\n",
    "The value of $m$ must be >= $r$ in order to uniquely determine the $r$ parity bits.\n",
    "Often $m = r$, as is the case for the LDPC code used by the FT8 data mode.\n",
    "For each equation which corresponds to a matrix row, there is a one in the matrix column where a bit in the codeword is included in the equation and a zero in the matrix column where the bit is not included in the equation.\n",
    "\n",
    "For LDPC codes, the parity-check matrix $H$ is sparse (i.e. contains many zeros) and therefore has a low-density of ones, which is where the \"low-density\" part of the LDPC name comes from.\n",
    "\n",
    "There are many possible LDPC codes, that can each be defined by block size $k$, codeword size $n$\n",
    "and parity-check matrix $H$.\n",
    "System designers choose values for $k$, $n$ and $H$ in order to meet the performance objectives\n",
    "of the system they are designing.\n",
    "\n",
    "The designers of the FT8 digital mode have choosen a specific LDPC code with $k=91$ and $n=174$\n",
    "and a parity-check matrix that is documented in the WSJT-X source code in the `lib/ft8/ldpc_174_91_c_parity.f90` compilation unit.\n",
    "There are $83$ parity-check equations and each equation only includes $6$ or $7$ bits from the $174$ bit\n",
    "codeword.\n",
    "The parity-check matrix $H$ is therefore quite sparse.\n",
    "\n",
    "The $91$ information bits in each block correspond to a single FT8 message that is first packed into $77$ bits,\n",
    "to which a $14$ bit cyclic redundancy check outer code is then added.\n",
    "The LDPC encoder then adds a further $83$ redundant parity bits to create a $174$ bit codeword for transmission.\n",
    "\n",
    "The rate of the resulting LDPC code is $91/174$ or slightly more than $1/2$.\n",
    "The rate of the combined LDPC code and CRC outer code is $77/174$ or slightly less than $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDPC Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of the encoder is to determine the $r$ parity bits from the $k$ information bits so that the a $n$ bit codeword can be constructed and passed onto the modulator for transmission across the communications channel.\n",
    "\n",
    "For LDPC codes, this can be achieved by solving the parity check equations for the $r$ parity bits.\n",
    "This can be done by starting with the parity check matrix $H$ and using linear algebra methods to obtain a generator matrix, which is typically denoted by the symbol $G$ in the literature.\n",
    "The generator matrix $G$ has $r$ rows and $k$ columns and represents $r$ equations, one for each of the parity bits that must be generated.\n",
    "\n",
    "Once the generator matrix $G$ has been determined for a particular LDPC code, the encoding step can be performed for each block by multiplying the generator matrix $G$ by a column vector containing the $k$ information bits in that block.\n",
    "\n",
    "The FT8 protocol implementation in the WSJT-X program performs LDPC encoding in the `lib/ft8/encode174_91.f90` compilation unit, using a pre-calculated generator matrix $G$ that has $83$ rows and $91$ columns.\n",
    "The pre-calculated generator matrix can be found in the `lib/ft8/ldpc_174_91_c_generator.f90` compilation unit.\n",
    "The generator matrix $G$ is expressed as an array of $83$ hexadecimal strings, one for each row in the generator matrix $G$.\n",
    "Each hexadecimal string has $23$ hex digits which is sufficient to represent $92$ bits.\n",
    "As there are only $91$ columns in the generator matrix $G$, the last bit in each hexadecimal string is not used and is therefore always a zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDPC Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDPC decoder is responsible for taking the output of the demodulator and determine the original $k$\n",
    "information bits with as few errors as possible.\n",
    "\n",
    "There are multiple methods that have been discovered for constructing LDPC decoders.\n",
    "The decoding methods fall into two broad categories that depend on the type of input available to the decoder.\n",
    "\n",
    "1. Hard decision decoders\n",
    "1. Soft decision decoders\n",
    "\n",
    "Hard decision decoders start with $n$ bit code words i.e. a hard decision has already been made by the detector\n",
    "at the output of the demodulation process as to whether each bit in the codeword is a 1 or a 0 before the decoder starts its work to determine the $k$ information bits.\n",
    "\n",
    "Soft decision decoders instead start with statistical probabilities that each of the $n$ codeword bits are a $0$ or $1$ and can typically provide the same error rate at lower signal to noise ratios than hard decision decoders.\n",
    "However, this improvement comes at the cost of increased complexity.\n",
    "\n",
    "Where sufficient computational capability exists to handle the complexity, soft decision decoders are generally preferred as they allow successful communication at lower signal to noise ratios i.e. the reduce the minimum signal to noise ratio that is required for near error free communication, typically by a few dB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelyhood Ratio\n",
    "\n",
    "A statistic used as input to many soft decision decoders for LDPC codes is the log likelyhood ratio or LLR.\n",
    "\n",
    "The LLR (as used in WSJT-X) is defined by the equation:\n",
    "\n",
    "$${LLR}_n = \\ln{(p_n(v|1)/p_n(v|0))} = \\ln{(p_n(v|1))} - \\ln{(p_n(v|0))}$$\n",
    "\n",
    "where $p_n(v|0)$ is the probability density function of the received symbol observations $v$, \n",
    "conditional on the $nth$ bit being 0,\n",
    "and $p_n(v|1)$ is the probability density function of the received symbol observations $v$,\n",
    "conditional on the $nth$ bit being 1.\n",
    "\n",
    "The LLR is an expression of confidence that the transmitted bit is a 1 or 0.\n",
    "Large positive values suggest a strong likelihood that the transmitted bit was a 1.\n",
    "Large negative values suggest a strong likelihood that the transmitted bit was a 0.\n",
    "\n",
    "Note that some literature on the LLR uses a different definition which swaps 0 and 1.\n",
    "In practical terms these definitions differ only in sign i.e. one definition is the negative of the other.\n",
    "In this analysis we will use the same convention used by the WSJT-X source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-Product Algorithm\n",
    "\n",
    "Many soft decision decoders for LDPC codes use variants of an iterative message passing technique\n",
    "called the sum-product algorithm.\n",
    "Gallager's original 1960s work proposed a LDPC decoder that is one such variant.\n",
    "\n",
    "The sum-product algorithm is also known by the name of \"belief propagation\", \n",
    "and this is the name used within the WSJT-X source code,\n",
    "where the algorithm is implemented in the  `lib/ft8/bpdecode171_91.f90` compilation unit.\n",
    "\n",
    "The tutorial paper by Kschischang et. al. provides a good history of the sum-product algorithm\n",
    "and its applicability to a broad range of problems including the decoding of LDPC codes\n",
    "[[5]](#References).\n",
    "Key points from this paper are summarised below.\n",
    "\n",
    "#### Factor Graphs\n",
    "\n",
    "The sum-product algorithm is most easily described using a concept called the factor graph.\n",
    "\n",
    "Factor graphs were conceived in an attempt to unify similar concepts that had\n",
    "independently evolved across multiple disciplines.\n",
    "They are a generalization of Tanner graphs, which had previously been used to describe the sum-product\n",
    "algorithm in the information theory literature.\n",
    "\n",
    "Factor graphs are bipartate graphs that can be used to represent many different types kinds of codes,\n",
    "of which LDPC codes are an example.\n",
    "\n",
    "Bipartate graphs are graphs comprised of two sets of nodes, such that graph edges only exist between nodes\n",
    "in different sets i.e. nodes in the same set can not be directly connected by an edge.\n",
    "\n",
    "When factor graphs are used to represent LDPC codes, the two sets of nodes in the factor graph are:\n",
    "\n",
    "1. Bit (variable) nodes which correspond to the bits in the LDPC codeword.\n",
    "For the FT8 digital mode, there are 174 bit nodes in the factor graph\n",
    "corresponding to the 174 bits in each LDPC codeword.\n",
    "2. Check (factor) nodes which correspond to the parity check equations in the LDPC code parity check matrix.\n",
    "For the FT8 digital mode, there are 83 check nodes in the factor graph corresponding to the 83 parity\n",
    "check equations.\n",
    "\n",
    "Where a parity check equation includes a particular bit position, the factor graph has an edge between the bit node that corresponds to the bit postition and the check node corresponding to the\n",
    "parity check equation.\n",
    "The LDPC code used by the FT8 digital mode has 522 edges connecting bit nodes with check nodes.\n",
    "There are 3 edges connected to each symbol node, and either 6 or 7 edges connected to each parity check node.\n",
    "\n",
    "This LDPC code is called an irregular LDPC code because the number of edges connected to each check\n",
    "node is not the same for all check nodes.\n",
    "\n",
    "WSJT-X represents the factor graph for the LDPC code used by the FT8 digital mode using definitions contained\n",
    "in the `lib/ft8/ldpc_174_91_c_reordered_parity.f90` compilation unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Description\n",
    "\n",
    "The sum-product algorithm is a message passing algorithm, \n",
    "which operates by exchanging messages between nodes along the edges of a factor graph.\n",
    "When the algorithm is applied to decoding a LDPC code, the factor graph is constructed\n",
    "using the parity check equations for the LDPC code as described previously.\n",
    "\n",
    "The algorithm starts by assigning values to each of the bit nodes.\n",
    "The values assigned to each bit node are the log likelyhood ratios (LLRs) that were calculated\n",
    "by the detector for each of the corresponding bits in the LDPC codeword.\n",
    "\n",
    "For factor graphs that contain cycles (as is usually the case for LDPC codes), the algorthm is iterative.\n",
    "During each iteration, bit nodes send messages to the check nodes to which they are connected on the factor\n",
    "graph, and the check nodes then send messages back to the bit nodes to which they are connected on the\n",
    "factor graph.\n",
    "\n",
    "At each node a calculation is performed on the incoming messages to generate outgoing messages.\n",
    "A slightly different calculation is performed to generate each outgoing message, depending on the destination\n",
    "node for that outgoing message.\n",
    "More specifically, an outgoing message destined to a node is based only on incoming messages\n",
    "from other nodes i.e. the prior incoming message from the destination node is excluded from the calculation\n",
    "of the outgoing message to that node.\n",
    "\n",
    "At each bit node the outgoing message to check node $i$ is calculated by adding the incoming\n",
    "messages from all check nodes other than check node $i$ to the initial LLR for bit node.\n",
    "\n",
    "These messages can be considered to be an improved estimate of the LLR for the bit corresponding to the bit\n",
    "node using corrections from the check nodes other than the check node to which the message is being sent.\n",
    "\n",
    "In the first iteration of the algorithm there are no incoming messages at the bit nodes, and the outgoing\n",
    "messages from each bit node are just the LLR that was initially assigned to that bit node i.e. the check nodes\n",
    "connected to a bit node on the factor graph are all sent the same uncorrected LLR as the message.\n",
    "\n",
    "At the check nodes the outgoing messages to bit node $j$ are calculated from the\n",
    "incoming messages from all the other bit nodes using the formula:\n",
    "\n",
    "$$outmsg_j = -2 \\arctan{\\left(\\prod_{k \\neq j} \\tan{\\left( -inmsg_k/2 \\right)} \\right)}$$\n",
    "\n",
    "Note that in some of the information theory literature a slightly different form of the formula is stated\n",
    "that excludes the minus signs.\n",
    "This is due to differences in how the LLR is defined in that literature.\n",
    "The form used here is that used in the WSJT-X source code.\n",
    "\n",
    "The message sent by the check nodes can be considered to be a correction for the destination bit node.\n",
    "The correction is based on the corrected LLRs received from other bit nodes in the same parity check equation.\n",
    "\n",
    "At the start of each iteration the decoder checks if a valid LDPC codeword can be obtained from a hard\n",
    "detection of the corrected LLR at each bit node.\n",
    "The corrected LLR used for hard detection includes corrections from all connected check nodes.\n",
    "A positive corrected LLR is detected as a 1 and a negative corrected LLR is detected as a 0.\n",
    "\n",
    "The validity of a corrected codeword is checked using the parity check equations for the LDPC code.\n",
    "The codeword is valid if all the parity check equations are satisfied.\n",
    "If the codeword is valid the sum-product algorithm would normally terminate at that point having\n",
    "successfully found a valid LDPC codeword.\n",
    "\n",
    "However, in the WSJT-X version of the algorithm, an additional test is performed on the codeword using\n",
    "the cyclic redundancy check (CRC) that was appended to the original message before it was encoded.\n",
    "If the CRC fails the algorithm continues in the hope that a different valid LDPC codeword will\n",
    "subsequently be found that does pass the CRC.\n",
    "If the CRC passes the algorithm terminates with a high level of confidence that an error free message\n",
    "has been successfully decoded.\n",
    "\n",
    "To avoid the algorithm iterating indefinitely, the algorithm also terminates after a maximum interation\n",
    "count is reached.\n",
    "\n",
    "A further optimisation used by WSJT-X is to terminate the algorithm before the maximum iteration\n",
    "count if it is determined that the algorithm is not converging on a solution.\n",
    "This determination is made using an empirical criteria based on the number of unsatisfied parity equations\n",
    "and how many times this number increased over an iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Statistics Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WSJT-X includes a second soft decision decoder was originally developed in the mid-1990s,\n",
    "and is known as the ordered statistics decoder [[6](#References)].\n",
    "\n",
    "This decoder is only used by WSJT-X when \"deep\" decoding is enabled in an attempt to decode signals that\n",
    "have not been decoded by the sum-product algorithm.\n",
    "\n",
    "The order statistics decoder will not be explored further in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRC Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FT8 digital mode uses a CRC (Cyclic Redundancy Check) code in addition to the LDPC code.\n",
    "While LDPC codes can both detect and correct errors, CRC codes are generally only used to detect errors.\n",
    "At very low signal to noise ratios, there is a significant non-zero probability that some errors will not be detected and corrected by the LDPC code used by the FT8 digital mode.\n",
    "The use of an additional CRC outer code enable detection of many such errors.\n",
    "\n",
    "Messages found by the CRC code to contain errors are discarded, as the CRC code is not capable of error correction.\n",
    "The FT8 digital mode will usually retransmit messages until they are successfully acknowledged, so a discarded message will eventually be delivered if the signal to noise ratio is sufficient to allow subsequent delivery of an error free message.\n",
    "\n",
    "CRC codes are derived from work on cylic error correcting codes in the late 1950s,\n",
    "and their applicability to error detection was popularized by Peterson and Brown in 1961 [[7](#References)].\n",
    "These codes have been widely used ever since, particularly in computer communicaton networks where the codes are part of many pervasive computer communication standards e.g. Ethernet.\n",
    "They are popular primarily because they can easily be implemented in hardware.\n",
    "\n",
    "Like LDPC codes, CRC codes take a block of message bits and append redundant parity bits to the message bits to create a codeword that is subsequently transmitted using an appropriate modulation method.\n",
    "The parity bits in CRC codes are the remainder of a polynomial division operation modulo 2,\n",
    "where the dividend is a polynomial derived from the block of message bits,\n",
    "and the divisor is a polynomial that is specific to a particular CRC code.\n",
    "\n",
    "The FT8 data mode uses a $14$ bit CRC code i.e. $14$ redundant parity bits are appended to each block of $77$ message bits to create a $91$ bit codeword.\n",
    "The $14$ bit CRC is obtained from the remainder of dividing modulo 2 a polynomial derived from the block of $77$\n",
    "message bits, by the polynomial:\n",
    "\n",
    "$$x^{14}+x^{13}+x^{10}+x^9+x^8+x^6+x^4+x^2+x^1+1$$\n",
    "\n",
    "CRC polynomials are often described in hexadecimal after dropping either the first or last term as those terms are always one.\n",
    "The boost C++ library (used by the WSJT-X program to generate and validate the CRC code)\n",
    "assumes the first term has been dropped and expresses the FT8 CRC polynomial in hexadecimal as `0x2757`.\n",
    "Literature that drops the last +1 term expresses the same polynomial as `0x33ab`.\n",
    "\n",
    "The input to the CRC code is the $77$ bit packed message padded with $19$ trailing zeros to create an array\n",
    "of $12$ bytes in big endian order.\n",
    "\n",
    "Once the CRC is calculated, it is appended to the $77$ bit packed message to create $91$ bits for subsequent LDPC encoding.\n",
    "\n",
    "Upon receipt, the $77$ bit packed message and $14$ bit CRC are extracted from the first $91$ bits of a successfully decoded LDPC codeword.\n",
    "The $77$ bit message is padded with $19$ trailing zeros and the CRC calculated.\n",
    "If it matches the received CRC the message is assumed to be error free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gray Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gray codes are named after Frank Gray who first described them in a 1947 patent application [[8](#References)].\n",
    "Gray codes are generally used to minimise bit errors when converting from the analog to digital domain.\n",
    "The intent of Gray codes is to choose a mapping between analog and digital representations so that small\n",
    "errors in the analog domain only cause a minimal number of bit errors in the digital domain.\n",
    "\n",
    "The FT8 digital mode uses M-GFSK modulation where $M$ is $8$.\n",
    "This form of modulation conveys symbols rather than individual bits - there are $M$ possible symbols\n",
    "transmitted, where each symbol represents $\\log_2{M}$ bits.\n",
    "In FT8 there are $8$ possible symbols, so each symbol conveys $\\log_2{8} = 3$ bits.\n",
    "\n",
    "In M-GFSK modulation, each of the possible symbols are transmitted using one of $M$ possible tones (frequencies).\n",
    "The Gray code used by FT8 defines the mapping between symbol values and transmitted tones\n",
    "i.e. it determines what tone is transmitted for each of the possible symbol values.\n",
    "The Gray code is therefore a simple permutation of the $M$ possible symbol values.\n",
    "Unlike the LDPC and CRC codes, Gray codes do not add any redundancy.\n",
    " \n",
    "The Gray code used by FT8 is defined in the `lib/ft8/genft8.f90` compilation unit as the permutation\n",
    "$[0, 1, 3, 2, 5, 6, 4, 7]$.\n",
    "At the receiver this becomes the following mapping from tones to bits:\n",
    "\n",
    "$$[0, 1, 2, 3, 4, 5, 6, 7] \\Rightarrow [000, 001, 011, 010, 110, 100, 101, 111]$$\n",
    "\n",
    "The key aspect of this mapping is that adjacent tones differ in the eventual binary representation\n",
    "by only one bit.\n",
    "This means that if a tone is detected incorrectly as an adjacent tone, it only causes a single bit error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Marcel J. E. Golay, \"Notes on Digital Coding,\" Proceedings of the IRE, vol. 37, p. 657, Jun. 1949.\n",
    "1. R. W. Hamming, \"Error Detecting and Error Correcting Codes,\" The Bell System Technical Journal, vol. 29, pp. 147-160, Apr. 1950.\n",
    "1. R. G. Gallager, \"Low-Density Parity-Check Codes,\" IRE Transactions on Information Theory, vol. 8, pp. 21-28, Jan. 1962.\n",
    "1. David J. C. MacKay and Radford M. Neal, \"Near Shannon Limit Performance of Low Density Parity Check Codes,\" Electronics Letters, vol. 33, pp. 457-458, Mar. 1997.\n",
    "1. Frank R. Kschischang, Brendan J. Frey and Hans-Andrea Loeliger, \"Factor Graphs and the Sum-Product Algorithm\", IEEE Transactions on Information Theory, vol. 47, pp. 498-519, Feb. 2001.\n",
    "1. Marc P. C. Fossorier and Shu Lin, \"Soft-Decision Decoding of Linear Block Codes Based on Ordered Statistics\", IEEE Transactions on Information Theory, vol. 41, pp. 1379-1396, Sep. 1995.\n",
    "1. W. W. Peterson and D. T. Brown, \"Cyclic Codes for Error Detection,\", Proceedings of the IRE, vol. 49, pp. 228-235, Jan. 1961.\n",
    "1. Frank Gray, \"Pulse Code Communication,\" U.S. Patent 2,632,058, filed 13 Nov. 1947, issued 17 Mar. 1953."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "Copyright (C) 2019 James Kelly, VK3JPK.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
